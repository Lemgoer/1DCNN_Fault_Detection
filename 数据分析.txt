dict_items([
('__header__', b'MATLAB 5.0 MAT-file, Platform: PCWIN, Created on: Mon Jan 31 15:28:20 2000'), 
('__version__', '1.0'), 
('__globals__', []), 

('X097_DE_time', 
array([[ 0.05319692],
       [ 0.08866154],
       [ 0.09971815],
       ...,
       [-0.03463015],
       [ 0.01668923],
       [ 0.04693846]])
       ), 

('X097_FE_time', 
array([[0.14566727],
       [0.09779636],
       [0.05485636],
       ...,
       [0.14053091],
       [0.09553636],
       [0.09019455]])
       ),

('X097RPM', array([[1796]], dtype=uint16))
])


(5215, 2048) (5215,) (1332, 2048) (1332,)

signal = value
print(signal.shape)  (243938, 1)
sample_num = signal.shape[0] // dim 243928/400=609

train_num = int(sample_num * train_fraction) 609*0.8
test_num = sample_num - train_num

signal = signal[0:dim * sample_num] [0:400*609]
# 按sample_num切分
signals = np.array(np.split(signal, sample_num)) (609, 400, 1)

signals_tr.append(signals[0:train_num, :]) 110*0.8
signals_tt.append(signals[train_num:sample_num, :])
labels_tr.append(idx * np.ones(train_num))
labels_tt.append(idx * np.ones(test_num))


(1218, 400) (1218,) (306, 400) (306,)
(1218, 400) (1218,) (306, 400) (306,)


----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv1d-1             [-1, 27, 2048]           1,512
       BatchNorm1d-2             [-1, 27, 2048]              54
              ReLU-3             [-1, 27, 2048]               0
         MaxPool1d-4              [-1, 27, 128]               0
            Conv1d-5              [-1, 27, 128]          40,122
       BatchNorm1d-6              [-1, 27, 128]              54
              ReLU-7              [-1, 27, 128]               0
           Dropout-8              [-1, 27, 128]               0
            Conv1d-9              [-1, 27, 128]          40,122
      BatchNorm1d-10              [-1, 27, 128]              54
             ReLU-11              [-1, 27, 128]               0
        MaxPool1d-12                [-1, 27, 8]               0
           Conv1d-13                [-1, 27, 8]          40,122
      BatchNorm1d-14                [-1, 27, 8]              54
             ReLU-15                [-1, 27, 8]               0
          Dropout-16                [-1, 27, 8]               0
           Conv1d-17                [-1, 27, 8]          40,122
      BatchNorm1d-18                [-1, 27, 8]              54
             ReLU-19                [-1, 27, 8]               0
          Flatten-20                  [-1, 216]               0
           Linear-21                  [-1, 101]          21,917
================================================================
Total params: 184,187
Trainable params: 184,187
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.70
Estimated Total Size (MB): 2.20
----------------------------------------------------------------



[ 5.31969231e-02,  8.86615385e-02,  9.97181538e-02,  5.86209231e-02,
       -4.58953846e-03, -5.69520000e-02, -7.17636923e-02, -5.86209231e-02,
       -4.65212308e-02, -4.98590769e-02, -5.11107692e-02, -1.56461538e-02,
        4.58953846e-02,  9.22080000e-02,  9.17907692e-02,  6.04984615e-02,
        2.44080000e-02, -2.08615385e-04,  1.75236923e-02,  2.62855385e-02,
       -3.54646154e-03, -4.10972308e-02, -6.23760000e-02, -3.25440000e-02,
        5.00676923e-03,  2.85803077e-02,  4.38092308e-02,  4.08886154e-02,
        5.52830769e-02,  8.53236923e-02,  1.08480000e-01,  9.90923077e-02,
        5.88295385e-02,  3.15009231e-02,  1.96098462e-02,  2.94147692e-02,
        6.02898462e-02,  7.57273846e-02,  6.69655385e-02,  5.90381538e-02,
        6.40449231e-02,  7.00947692e-02,  6.78000000e-02,  4.44350769e-02,
        2.10701538e-02,  1.73150769e-02,  4.13058462e-02,  9.65889231e-02,
        1.40815385e-01,  1.50411692e-01,  1.39980923e-01,  1.20162462e-01,
        9.90923077e-02,  8.15686154e-02,  5.19452308e-02,  6.04984615e-03,
       -3.81766154e-02, -5.17366154e-02, -2.14873846e-02,  2.06529231e-02,
        4.48523077e-02,  5.86209231e-02,  6.59224615e-02,  7.17636923e-02,
        7.23895385e-02,  3.10836923e-02, -3.98455385e-02, -1.24752000e-01,
       -1.83164308e-01, -1.73359385e-01, -1.36225846e-01, -6.52966154e-02,
        4.58953846e-03,  5.82036923e-02,  9.86750769e-02,  7.23895385e-02,
        1.81495385e-02, -6.13329231e-02, -1.48951385e-01, -1.95889846e-01,
       -1.98601846e-01, -1.33513846e-01, -4.86073846e-02,  3.02492308e-02,
        8.24030769e-02,  1.09105846e-01,  1.25795077e-01,  9.17907692e-02,
        3.46301538e-02, -4.44350769e-02, -1.10774769e-01, -1.27464000e-01,
       -1.06602462e-01, -4.63126154e-02,  5.42400000e-03,  4.02627692e-02,
        8.63667692e-02,  1.24960615e-01,  1.34974154e-01,  9.84664615e-02,
        2.02356923e-02, -6.98861538e-02, -1.31219077e-01, -1.33931077e-01,
       -8.38633846e-02,  1.04307692e-03,  6.25846154e-02,  1.00969846e-01,
        1.44361846e-01,  1.68352615e-01,  1.54584000e-01,  8.44892308e-02,
       -2.19046154e-02, -1.08897231e-01, -1.32470769e-01, -9.82578462e-02,
       -3.52560000e-02,  5.09021538e-02,  1.29967385e-01,  1.85459077e-01,
        2.17794462e-01,  2.09658462e-01,  1.61051077e-01,  8.74098462e-02,
        1.16824615e-02, -4.06800000e-02, -3.50473846e-02,  9.17907692e-03,
        4.56867692e-02,  7.67704615e-02,  9.76320000e-02,  1.14947077e-01,
        1.35600000e-01,  1.17033231e-01,  6.11243077e-02, -1.87753846e-02,
       -8.86615385e-02, -9.65889231e-02, -7.09292308e-02, -2.48252308e-02,
        2.58683077e-02,  5.31969231e-02,  7.19723077e-02,  8.28203077e-02,
        7.80221538e-02,  4.46436923e-02, -2.73286154e-02, -1.18910769e-01,
       -1.76905846e-01, -1.78783385e-01, -1.48116923e-01, -9.28338462e-02,
       -3.92196923e-02,  1.46030769e-03,  4.38092308e-02,  8.07341538e-02,
        7.57273846e-02,  1.89840000e-02, -6.11243077e-02, -1.34556923e-01,
       -1.68769846e-01, -1.55627077e-01, -1.27672615e-01, -1.02221538e-01,
       -7.05120000e-02, -4.92332308e-02, -3.54646154e-02, -4.33920000e-02,
       -7.57273846e-02, -1.18076308e-01, -1.61259692e-01, -1.63971692e-01,
       -1.19119385e-01, -4.63126154e-02,  1.75236923e-02,  5.19452308e-02,
        7.34326154e-02,  8.63667692e-02,  7.48929231e-02,  5.40313846e-02,
        1.06393846e-02, -4.25575385e-02, -5.36141538e-02, -4.17230769e-02,
       -9.80492308e-03,  1.73150769e-02,  4.46436923e-02,  7.36412308e-02,
        9.30424615e-02,  1.24960615e-01,  1.33096615e-01,  1.07436923e-01,
        5.88295385e-02,  1.60633846e-02,  1.60633846e-02,  5.29883077e-02,
        8.53236923e-02,  1.00135385e-01,  1.05976615e-01,  1.03264615e-01,
        9.38769231e-02,  6.15415385e-02,  1.46030769e-02, -5.34055385e-02,
       -1.18910769e-01, -1.37060308e-01, -9.45027692e-02, -2.06529231e-02,
        3.54646154e-02,  8.55323077e-02,  1.32053538e-01,  1.62511385e-01,
        1.59382154e-01,  1.19745231e-01,  5.98726154e-02, -2.16960000e-02,
       -8.84529231e-02, -1.13278154e-01, -9.78406154e-02, -5.82036923e-02,
       -2.14873846e-02,  1.96098462e-02,  6.21673846e-02,  8.61581538e-02,
        9.17907692e-02,  6.34190769e-02,  1.23083077e-02, -2.19046154e-02,
       -3.77593846e-02, -4.02627692e-02, -3.92196923e-02, -3.98455385e-02,
       -4.27661538e-02, -3.65076923e-02, -2.23218462e-02, -1.98184615e-02,
       -3.33784615e-02, -6.61310769e-02, -9.59630769e-02, -1.05976615e-01,
       -8.65753846e-02, -4.29747692e-02, -2.27390769e-02, -8.34461538e-03,
        2.39907692e-02,  5.67433846e-02,  8.13600000e-02,  6.52966154e-02,
        1.50203077e-02, -3.48387692e-02, -6.32104615e-02, -4.54781538e-02,
        6.25846154e-03,  5.75778462e-02,  8.84529231e-02,  9.59630769e-02,
        1.06602462e-01,  1.27881231e-01,  1.28089846e-01,  1.01387077e-01,
        5.65347692e-02,  1.54375385e-02,  1.48116923e-02,  5.82036923e-02,
        9.24166154e-02,  1.00552615e-01,  1.02221538e-01,  1.09523077e-01,
        1.38729231e-01,  1.58756308e-01,  1.45822154e-01,  9.80492308e-02,
        3.44215385e-02, -8.55323077e-03, -2.19046154e-02, -2.10701538e-02,
       -1.58547692e-02, -1.12652308e-02, -2.71200000e-03,  3.62990769e-02,
        8.86615385e-02,  1.24334769e-01,  1.14321231e-01,  7.07206154e-02,
        1.50203077e-02, -2.81630769e-02, -8.55323077e-03,  2.67027692e-02,
        3.27526154e-02,  2.81630769e-02,  2.83716923e-02,  5.38227692e-02,
        7.65618462e-02,  5.90381538e-02,  1.41858462e-02, -3.88024615e-02,
       -5.90381538e-02, -3.85938462e-02, -8.97046154e-03,  1.25169231e-03,
       -1.29341538e-02, -2.27390769e-02, -1.08480000e-02,  2.16960000e-02,
        3.42129231e-02,  3.96369231e-03, -5.02763077e-02, -9.11649231e-02,
       -8.59495385e-02, -7.13464615e-02, -6.32104615e-02, -5.98726154e-02,
       -7.34326154e-02, -6.67569231e-02, -3.62990769e-02, -5.42400000e-03,
        1.12652308e-02, -4.79815385e-03, -3.17095385e-02, -3.83852308e-02,
       -2.19046154e-02, -6.25846154e-04,  1.25169231e-02,  2.27390769e-02,
        4.10972308e-02,  6.80086154e-02,  8.49064615e-02,  9.59630769e-02,
        7.98996923e-02,  1.52289231e-02, -3.85938462e-02, -3.83852308e-02,
       -5.00676923e-03,  1.73150769e-02,  1.46030769e-02,  2.94147692e-02,
        6.59224615e-02,  9.51286154e-02,  1.17241846e-01,  1.05768000e-01,
        6.96775385e-02,  3.17095385e-02,  1.23083077e-02,  3.71335385e-02,
        5.59089231e-02,  4.54781538e-02,  3.27526154e-02,  3.27526154e-02,
        4.33920000e-02,  4.31833846e-02,  4.23489231e-02,  3.15009231e-02,
       -2.29476923e-03, -1.68978462e-02,  8.13600000e-03,  4.21403077e-02,
        6.38363077e-02,  7.51015385e-02,  7.25981538e-02,  8.69926154e-02,
        1.03681846e-01,  9.17907692e-02,  4.17230769e-02, -4.10972308e-02,
       -9.72147692e-02, -1.02847385e-01, -7.28067692e-02, -3.29612308e-02,
       -4.58953846e-03,  1.08480000e-02,  3.62990769e-02,  7.09292308e-02,
        8.59495385e-02,  7.63532308e-02,  3.79680000e-02, -2.35735385e-02,
       -6.38363077e-02, -5.63261538e-02, -3.17095385e-02, -1.91926154e-02,
       -2.31563077e-02, -2.64941538e-02, -8.55323077e-03,  4.13058462e-02,
        6.80086154e-02,  4.61040000e-02,  7.71876923e-03, -1.66892308e-02,
        7.30153846e-03,  4.38092308e-02,  7.13464615e-02,  8.44892308e-02,
        7.05120000e-02,  6.94689231e-02,  8.94960000e-02,  1.04307692e-01,
        7.73963077e-02,  5.21538462e-03, -4.38092308e-02, -6.13329231e-02]), 0)




F:\Anaconda3\envs\Pytorch\python.exe F:/working_space/1D-CNN_Fault_Detection/main.py
Source path:... F:/working_space/1D-CNN_Fault_Detection/main.py
10:39:20.873570 call        26 def train():
10:39:20.874535 line        35     model = getattr(models, opt.model)()
    Source path:... F:\working_space\1D-CNN_Fault_Detection\models\CWRUcnn.py
    Starting var:.. self = REPR FAILED
    Starting var:.. kernel1 = 27
    Starting var:.. kernel2 = 36
    Starting var:.. kernel_size = 10
    Starting var:.. pad = 0
    Starting var:.. ms1 = 4
    Starting var:.. ms2 = 4
    Starting var:.. __class__ = <class 'models.CWRUcnn.CWRUcnn'>
    10:39:20.874535 call        27     def __init__(self, kernel1=27, kernel2=36, kernel_size=10, pad=0, ms1=4, ms2=4):
    10:39:20.874535 line        28         super(CWRUcnn, self).__init__()
    Modified var:.. self = CWRUcnn()
    10:39:20.874535 line        29         self.conv = nn.Sequential(
    10:39:20.874535 line        30             nn.Conv1d(1, kernel1, kernel_size, padding=pad),
    10:39:20.875561 line        31             nn.BatchNorm1d(kernel1),
    10:39:20.875561 line        32             nn.ReLU(),
    10:39:20.875561 line        33             nn.MaxPool1d(ms1),
    10:39:20.876562 line        34             nn.Conv1d(kernel1, kernel1, kernel_size, padding=pad),
    10:39:20.876562 line        35             nn.BatchNorm1d(kernel1),
    10:39:20.876562 line        36             nn.ReLU(),
    10:39:20.877633 line        37             nn.Dropout(),
    10:39:20.877633 line        38             nn.Conv1d(kernel1, kernel1, kernel_size, padding=pad),
    10:39:20.877633 line        39             nn.BatchNorm1d(kernel1),
    10:39:20.878593 line        40             nn.ReLU(),
    10:39:20.878593 line        41             nn.MaxPool1d(ms2),
    10:39:20.878593 line        42             nn.Conv1d(kernel1, kernel2, kernel_size, padding=pad),
    10:39:20.878593 line        43             nn.BatchNorm1d(kernel2),
    10:39:20.878593 line        44             nn.ReLU(),
    10:39:20.878593 line        45             nn.Dropout(),
    10:39:20.879588 line        46             nn.Conv1d(kernel2, kernel2, kernel_size, padding=pad),
    10:39:20.879588 line        47             nn.BatchNorm1d(kernel2),
    10:39:20.879588 line        48             nn.ReLU(),
    10:39:20.879588 line        49             Flatten()
    Modified var:.. self = CWRUcnn(  (conv): Sequential(    (0): Conv1d(1, ...tats=True)    (18): ReLU()    (19): Flatten()  ))
    10:39:20.879588 line        52         self.fc = nn.Sequential(
    10:39:20.880586 line        53             nn.Linear(27*8, 32),
    10:39:20.880586 line        54             nn.ReLU(),
    10:39:20.880586 line        55             nn.Linear(32, 4),
    Modified var:.. self = CWRUcnn(  (conv): Sequential(    (0): Conv1d(1, ...ar(in_features=32, out_features=4, bias=True)  ))
    10:39:20.881582 return      55             nn.Linear(32, 4),
    Return value:.. None
    Elapsed time: 00:00:00.007047
New var:....... model = CWRUcnn(  (conv): Sequential(    (0): Conv1d(1, ...ar(in_features=32, out_features=4, bias=True)  ))
10:39:20.881582 line        36     if opt.load_model_path:
10:39:20.881582 line        38     if opt.use_gpu: model.cuda()
10:39:22.260439 line        42     train_data = CWRUDataset(opt.train_data_root, train=True)
New var:....... train_data = <data.dataset.CWRUDataset object at 0x0000023D5869CF98>
10:39:22.263433 line        43     val_data = CWRUDataset(opt.val_data_root, train=False)
New var:....... val_data = <data.dataset.CWRUDataset object at 0x0000023D5CB5E7B8>
10:39:22.265392 line        45     train_dataloader = DataLoader(train_data, opt.batch_size, shuffle=True)
New var:....... train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x0000023D5869C5C0>
10:39:22.265392 line        46     val_dataloader = DataLoader(val_data, opt.batch_size, shuffle=False)
New var:....... val_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x0000023D586C5278>
10:39:22.266421 line        49     criterion = torch.nn.CrossEntropyLoss()
New var:....... criterion = CrossEntropyLoss()
10:39:22.266421 line        50     lr = opt.lr
New var:....... lr = 0.001
10:39:22.266421 line        51     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=opt.weight_decay)
New var:....... optimizer = Adam (Parameter Group 0    amsgrad: False    bet... eps: 1e-08    lr: 0.001    weight_decay: 0.0001)
10:39:22.267418 line        52     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, opt.lr_decay_iters,
10:39:22.267418 line        53                                                 opt.lr_decay)  # regulation rate decay
Modified var:.. optimizer = Adam (Parameter Group 0    amsgrad: False    bet...l_lr: 0.001    lr: 0.001    weight_decay: 0.0001)
New var:....... scheduler = <torch.optim.lr_scheduler.StepLR object at 0x0000023D5869C780>
10:39:22.267418 line        60     writer = SummaryWriter()
New var:....... writer = <tensorboardX.writer.SummaryWriter object at 0x0000023D586C5390>
10:39:22.269413 line        62     best_model_wts = copy.deepcopy(model.state_dict())
New var:....... best_model_wts = {'conv.0.weight': tensor<(27, 1, 10), float32, c...a:0>, 'fc.2.bias': tensor<(4,), float32, cuda:0>}
10:39:22.274400 line        63     best_acc = 0.0
New var:....... best_acc = 0.0
10:39:22.291354 line        65     device = ''
New var:....... device = ''
10:39:22.307156 line        66     if opt.use_gpu:
10:39:22.322132 line        67         use_cuda = torch.cuda.is_available()
New var:....... use_cuda = True
10:39:22.338089 line        68         if use_cuda:
CUDA is available
10:39:22.354018 line        69             print('CUDA is available')
10:39:22.369975 line        70             device = torch.device(opt.device)
Modified var:.. device = device(type='cuda', index=0)
10:39:22.385961 line        75     for epoch in range(opt.max_epoch):
New var:....... epoch = 0
10:39:22.401918 line        77         start_time = time.time()
New var:....... start_time = 1594348762.4174602
10:39:22.417460 line        78         print('Starting epoch %d / %d' % (epoch + 1, opt.max_epoch))
Starting epoch 1 / 50
10:39:22.433448 line        80         model.train()
10:39:22.449443 line        82         for ii, (data, label) in tqdm(enumerate(train_dataloader), total=len(train_data)):
  0%|          | 0/1218 [00:00<?, ?it/s]New var:....... ii = 0
New var:....... data = tensor<(32, 400), float64, cpu>
New var:....... label = tensor<(32,), uint8, cpu>
10:39:22.467463 line        86             data.resize_(data.size()[0], 1, data.size()[1])
Modified var:.. data = tensor<(32, 1, 400), float64, cpu>
10:39:22.486382 line        88             data, label = data.float(), label.long()
Modified var:.. data = tensor<(32, 1, 400), float32, cpu>
Modified var:.. label = tensor<(32,), int64, cpu>
10:39:22.504610 line        89             input, target = data.to(device), label.to(device)
New var:....... input = tensor<(32, 1, 400), float32, cuda:0>
New var:....... target = tensor<(32,), int64, cuda:0>
10:39:22.523477 line        98             optimizer.zero_grad()
10:39:22.545699 line        99             score = model(input)
    Source path:... F:\working_space\1D-CNN_Fault_Detection\models\CWRUcnn.py
    Starting var:.. self = CWRUcnn(  (conv): Sequential(    (0): Conv1d(1, ...ar(in_features=32, out_features=4, bias=True)  ))
    Starting var:.. x = tensor<(32, 1, 400), float32, cuda:0>
    10:39:22.563625 call        59     def forward(self, x):
    10:39:22.564643 line        60         x = self.conv(x)
    Modified var:.. x = tensor<(32, 36), float32, cuda:0, grad>
    10:39:23.307901 line        61         x = self.fc(x)
    10:39:23.309884 exception   61         x = self.fc(x)
    Exception:..... RuntimeError: size mismatch, m1: [32 x 36], m2: ...ork/aten/src\THC/generic/THCTensorMathBlas.cu:290
    Call ended by exception
    Elapsed time: 00:00:00.747257
10:39:23.310882 exception   99             score = model(input)
Exception:..... RuntimeError: size mismatch, m1: [32 x 36], m2: ...ork/aten/src\THC/generic/THCTensorMathBlas.cu:290
  0%|          | 0/1218 [00:00<?, ?it/s]
Call ended by exception
Elapsed time: 00:00:02.470226
Traceback (most recent call last):
  File "F:/working_space/1D-CNN_Fault_Detection/main.py", line 184, in <module>
    train()
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\pysnooper\tracer.py", line 263, in simple_wrapper
    return function(*args, **kwargs)
  File "F:/working_space/1D-CNN_Fault_Detection/main.py", line 99, in train
    score = model(input)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\pysnooper\tracer.py", line 263, in simple_wrapper
    return function(*args, **kwargs)
  File "F:\working_space\1D-CNN_Fault_Detection\models\CWRUcnn.py", line 61, in forward
    x = self.fc(x)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\container.py", line 100, in forward
    input = module(input)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "F:\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\functional.py", line 1370, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [32 x 36], m2: [216 x 32] at C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src\THC/generic/THCTensorMathBlas.cu:290

Process finished with exit code 1


testing length = 306
CUDA is available
测试时间 0.9575085639953613
测试时间 0.9614396095275879
测试时间 0.9624300003051758
测试时间 0.9903631210327148
测试时间 0.97336745262146
Got 305 / 306 correct (99.67)
0.96902174949645996
0.32900724983215332

testing length = 611
CUDA is available
测试时间 1.012317180633545
测试时间 0.9963064193725586
测试时间 0.9733972549438477
测试时间 1.001312494277954
测试时间 1.0093011856079102
Got 610 / 611 correct (99.84)
0.9985269069671631
0.3398423023223877

testing length = 916
CUDA is available
测试时间 1.011399507522583
测试时间 1.0341894626617432
测试时间 1.0313000679016113
测试时间 1.0342299938201904
测试时间 1.0272564888000488
Got 915 / 916 correct (99.89)
1.02767510414123534
0.34755836804707844666666666666667

testing length = 1221
CUDA is available
测试时间 1.0382297039031982
测试时间 1.0333504676818848
测试时间 1.0322506427764893
测试时间 1.0481994152069092
测试时间 1.053138256072998
Got 1220 / 1221 correct (99.92)
1.0410336971282959
0.35201123237609863333333333333333

testing length = 1524
CUDA is available
测试时间 1.0591754913330078
测试时间 1.0701401233673096
测试时间 1.0641329288482666
测试时间 1.0561745166778564
测试时间 1.072317123413086
Got 1523 / 1524 correct (99.93)
1.06438803672790528
0.36179601224263509333333333333333


signal 
(243938, 1)
(122136, 1)
(121991, 1)
(122426, 1)
[[ 0.05319692]
 [ 0.08866154]
 [ 0.09971815]
 ...
 [-0.03463015]
 [ 0.01668923]
 [ 0.04693846]]
[[ 1.18943124]
 [-0.17786647]
 [-0.77481557]
 ...
 [-0.03208094]
 [-0.27573363]
 [ 0.08649671]]
[[-0.00795932]
 [ 0.02533988]
 [ 0.00016244]
 ...
 [ 0.10558283]
 [-0.07829373]
 [-0.02306579]]
[[ 0.10436457]
 [ 0.01746178]
 [ 0.11654721]
 ...
 [ 0.13319681]
 [ 0.21075958]
 [-0.58639082]]

signal 修正后
(243920, 1)
(122120, 1)
(121960, 1)
(122400, 1)

signals
(6098, 40, 1)
(3053, 40, 1)
(3049, 40, 1)
(3060, 40, 1)

array([[-0.03650769],
       [-0.01001354],
       [ 0.02002708],
       [ 0.05799508],
       [ 0.08594954],
       [ 0.13455692],
       [ 0.17419385],
       [ 0.17273354],
       [ 0.132888  ],
       [ 0.04965046],
       [-0.00375508],
       [-0.04693846],
       [-0.06759138],
       [-0.05757785],
       [-0.05277969],
       [-0.02044431],
       [ 0.02419938],
       [ 0.08386338],
       [ 0.11599015],
       [ 0.092208  ],
       [ 0.05173662],
       [-0.024408  ],
       [-0.09304246],
       [-0.11870215],
       [-0.113904  ],
       [-0.089496  ],
       [-0.06279323],
       [-0.02253046],
       [ 0.02753723],
       [ 0.06571385],
       [ 0.05924677],
       [ 0.010848  ],
       [-0.05590892],
       [-0.10368185],
       [-0.10618523],
       [-0.08782708],
       [-0.05820369],
       [-0.03734215],
       [-0.01606338],
       [ 0.02148738]]),


[[[ 0.05319692]
  [ 0.08866154]
  [ 0.09971815]
  ...
  [ 0.07572738]
  [ 0.06696554]
  [ 0.05903815]]

 [[-0.03692492]
  [ 0.00458954]
  [ 0.01460308]
  ...
  [-0.04652123]
  [-0.10013538]
  [-0.12412615]]

 [[-0.00375508]
  [ 0.02044431]
  [ 0.03316985]
  ...
  [ 0.04380923]
  [ 0.05257108]
  [ 0.06446215]]

 ...

 [[-0.04756431]
  [-0.00876185]
  [ 0.02920615]
  ...
  [ 0.00730154]
  [ 0.05799508]
  [ 0.10994031]]

 [[ 0.08490646]
  [ 0.03567323]
  [-0.02670277]
  ...
  [-0.05528308]
  [-0.05215385]
  [-0.04380923]]

 [[-0.04819015]
  [ 0.00020862]
  [ 0.05590892]
  ...
  [ 0.03087508]
  [ 0.06675692]
  [ 0.06362769]]]

Process finished with exit code 0

split_data
(610, 40, 1)


(12197, 40) (12197,) (3063, 40) (3063,)


F:\Anaconda3\envs\Pytorch\python.exe F:/working_space/1D-CNN_Fault_Detection/data/data_preprocess.py
[array([[[ 0.05319692],
        [ 0.08866154],
        [ 0.09971815],
        ...,
        [ 0.10013538],
        [ 0.10597662],
        [ 0.10326462]],

       [[ 0.09387692],
        [ 0.06154154],
        [ 0.01460308],
        ...,
        [ 0.00521538],
        [-0.04380923],
        [-0.06133292]],

       [[-0.03692492],
        [ 0.00458954],
        [ 0.01460308],
        ...,
        [ 0.01731508],
        [-0.04276615],
        [-0.06112431]],

       ...,

       [[ 0.002712  ],
        [ 0.046104  ],
        [ 0.07656185],
        ...,
        [ 0.00625846],
        [ 0.06362769],
        [ 0.11807631]],

       [[ 0.14394462],
        [ 0.15792185],
        [ 0.16876985],
        ...,
        [ 0.08553231],
        [ 0.05111077],
        [ 0.00771877]],

       [[-0.04526954],
        [-0.04777292],
        [-0.03004062],
        ...,
        [ 0.09909231],
        [ 0.00709292],
        [-0.10055262]]])]
[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0.])]

Process finished with exit code 0




4
Running time: 0.0182227 Seconds
7
Running time: 0.030386700000000003 Seconds
9
Running time: 0.038014900000000004 Seconds
20
Running time: 0.0959202 Seconds




